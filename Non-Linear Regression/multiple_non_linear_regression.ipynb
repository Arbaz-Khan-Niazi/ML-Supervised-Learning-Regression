{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Non-Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will build a multiple non-linear regression model, we will use polynomial features to make our original features higher order polynomials, enabling the model to capture non-linearity in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [1 - Packages](#1)\n",
    "\n",
    "- [2 - Dataset](#2)\n",
    "\n",
    "- [3 - Multiple Non-Linear Regression Model Using Normal Equation](#3)\n",
    "    - [3.1 Custom Model NE](#3point1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1 - Packages\n",
    "\n",
    "Below are the packages/libraries that we are going to use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages/libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2 - Dataset\n",
    "\n",
    "We will going to use dataset from [Kaggle](https://www.kaggle.com/datasets/hamzakhurshed/concrete-strength-dataset).\n",
    "\n",
    "The dataset is downloaded and stored in `multiple_non_lr_dataset` folder. The folder contains a CSV file named **concrete_data.csv**, which we are going to use to train, validate and test our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the dataset into pandas dataframe, and get the overview of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>276.4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>90.3</td>\n",
       "      <td>179.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>870.1</td>\n",
       "      <td>768.3</td>\n",
       "      <td>28</td>\n",
       "      <td>44.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>322.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.6</td>\n",
       "      <td>196.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>817.9</td>\n",
       "      <td>813.4</td>\n",
       "      <td>28</td>\n",
       "      <td>31.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>148.5</td>\n",
       "      <td>139.4</td>\n",
       "      <td>108.6</td>\n",
       "      <td>192.7</td>\n",
       "      <td>6.1</td>\n",
       "      <td>892.4</td>\n",
       "      <td>780.0</td>\n",
       "      <td>28</td>\n",
       "      <td>23.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>159.1</td>\n",
       "      <td>186.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.6</td>\n",
       "      <td>11.3</td>\n",
       "      <td>989.6</td>\n",
       "      <td>788.9</td>\n",
       "      <td>28</td>\n",
       "      <td>32.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>260.9</td>\n",
       "      <td>100.5</td>\n",
       "      <td>78.3</td>\n",
       "      <td>200.6</td>\n",
       "      <td>8.6</td>\n",
       "      <td>864.5</td>\n",
       "      <td>761.5</td>\n",
       "      <td>28</td>\n",
       "      <td>32.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0      540.0                 0.0      0.0  162.0               2.5   \n",
       "1      540.0                 0.0      0.0  162.0               2.5   \n",
       "2      332.5               142.5      0.0  228.0               0.0   \n",
       "3      332.5               142.5      0.0  228.0               0.0   \n",
       "4      198.6               132.4      0.0  192.0               0.0   \n",
       "...      ...                 ...      ...    ...               ...   \n",
       "1025   276.4               116.0     90.3  179.6               8.9   \n",
       "1026   322.2                 0.0    115.6  196.0              10.4   \n",
       "1027   148.5               139.4    108.6  192.7               6.1   \n",
       "1028   159.1               186.7      0.0  175.6              11.3   \n",
       "1029   260.9               100.5     78.3  200.6               8.6   \n",
       "\n",
       "      Coarse Aggregate  Fine Aggregate  Age  Strength  \n",
       "0               1040.0           676.0   28     79.99  \n",
       "1               1055.0           676.0   28     61.89  \n",
       "2                932.0           594.0  270     40.27  \n",
       "3                932.0           594.0  365     41.05  \n",
       "4                978.4           825.5  360     44.30  \n",
       "...                ...             ...  ...       ...  \n",
       "1025             870.1           768.3   28     44.28  \n",
       "1026             817.9           813.4   28     31.18  \n",
       "1027             892.4           780.0   28     23.70  \n",
       "1028             989.6           788.9   28     32.77  \n",
       "1029             864.5           761.5   28     32.40  \n",
       "\n",
       "[1030 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset into pandas dataframe\n",
    "data = pd.read_csv(\"multiple_non_lr_dataset/concrete_data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have **1,030 rows** and **9 columns** in our dataset, from which we are going to split the dataset into **60%** for training, **20%** for cross-validation, and **20%** for testing. The first eight variables are feature variables $\\bf{x}$, and our target variable $y$ is last variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the dataset is clean, we will still going to look for any missing/NaN values in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cement                False\n",
       "Blast Furnace Slag    False\n",
       "Fly Ash               False\n",
       "Water                 False\n",
       "Superplasticizer      False\n",
       "Coarse Aggregate      False\n",
       "Fine Aggregate        False\n",
       "Age                   False\n",
       "Strength              False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for any missing values in a dataset\n",
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no missing/NaN values in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the dataset into numpy arrays, and split it into training, cross-validation and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1030, 8) & Shape of Y: (1030, 1)\n"
     ]
    }
   ],
   "source": [
    "# Converting pandas dataframe into numpy arrays\n",
    "X = data.to_numpy()[:, 0:8]\n",
    "Y = data.to_numpy()[:, 8:9]\n",
    "\n",
    "print(f\"Shape of X: {X.shape} & Shape of Y: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (618, 8) & Shape of Y_train: (618, 1)\n",
      "Shape of X_val: (206, 8) & Shape of Y_val: (206, 1)\n",
      "Shape of X_test: (206, 8) & Shape of Y_test: (206, 1)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training (60%) and temp (40%)\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Splitting temp into validation (20%) and test (20%)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape} & Shape of Y_train: {Y_train.shape}\\n\"\n",
    "      f\"Shape of X_val: {X_val.shape} & Shape of Y_val: {Y_val.shape}\\n\"\n",
    "      f\"Shape of X_test: {X_test.shape} & Shape of Y_test: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3 - Multiple Non-Linear Regression Model Using Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3point1\"></a>\n",
    "### 3.1 Custom Model NE\n",
    "\n",
    "Since we do not know the data has linear or non-linear characteristics, we will train linear and few non-linear models, we will use **PolynomialFeatures** method from Scikit-Learn to transform $\\bf{x}$ into higher-degree polynomial terms, capturing the non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal parameters for our model, we will use the **Gaussian Elimination method** to solve the below equation, instead of computing the inverse $(X^T \\cdot X)^{-1}$ directly (which can lead to numerical instability).\n",
    "\n",
    "$$ (X^T \\cdot X) \\cdot W = X^T \\cdot Y \\tag{1} $$\n",
    "\n",
    "where,\n",
    "- $W$ is a model parameters matrix.\n",
    "- $X$ is a input feature matrix.\n",
    "- $Y$ is a target variable matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**NOTE:** Solving regression problem using normal equation requires the bias term to be included in weight matrix. Therefore we also have to add 1's column vector to our original feature datasets (which will be automatically added when using **PolynomialFeatures** method), so the dimensions should be matched during matrix multiplication. And also feature scaling is not required because it does not use gradient descent to find the optimal parameters for the model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree_1 model -> train_cost: 102.79385145348093, val_cost: 114.38470630338443\n",
      "\n",
      "Degree_2 model -> train_cost: 49.537424005732774, val_cost: 61.950069371463734\n",
      "\n",
      "Degree_3 model -> train_cost: 17.16449479097078, val_cost: 49.10177017537962\n",
      "\n",
      "Degree_4 model -> train_cost: 4.110646166252671, val_cost: 1648597.3809762562\n",
      "\n",
      "Degree_5 model -> train_cost: 1.8545894416303743, val_cost: 138618349056.97867\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store models parameters, and polynomial transformers\n",
    "models = {}\n",
    "\n",
    "for degree in range(1, 6):  # Polynomial degrees 1 to 5 (degree 1 -> no polynomial features -> linear)\n",
    "\n",
    "    # Applying polynomial features to training and validation data\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "\n",
    "    # Finding model parameters using normal equation method\n",
    "    W = np.linalg.solve((X_train_poly.T @ X_train_poly), (X_train_poly.T @ Y_train))\n",
    "    \n",
    "    # Evaluating on transformed training and cross-validation data\n",
    "    Y_hat_train = X_train_poly @ W\n",
    "    Y_hat_val = X_val_poly @ W\n",
    "\n",
    "    # Calculating the cost\n",
    "    train_cost = np.mean((Y_hat_train - Y_train)**2)\n",
    "    val_cost = np.mean((Y_hat_val - Y_val)**2)\n",
    "\n",
    "    print(f\"Degree_{degree} model -> train_cost: {train_cost}, val_cost: {val_cost}\\n\")\n",
    " \n",
    "    # Storing the model parameters, and polynomial transformer\n",
    "    models[f\"degree_{degree}\"] = {\n",
    "        \"W\": W,\n",
    "        \"poly\": poly,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By seeing the above metrics, degree 2 is the best choice as it generalizes better. While Degree 3 has a lower training cost (**17.16** vs. **49.54** for Degree 2), its validation cost (**49.10**) is higher in range than Degree 2 (**61.95**), indicating possible overfitting. Degree 1 underfits (**train: 102.79, val: 114.38**), while Degrees 4 and 5 suffer from extreme overfitting. Thus, **Degree 2 offers the best balance** between bias and variance, making the dataset suitable for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make prediction on testing dataset using the optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the optimal model parameters and polynomial transformer\n",
    "W_2 = models[\"degree_2\"][\"W\"]\n",
    "poly_2 = models[\"degree_2\"][\"poly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_cost: 64.99201321803871\n"
     ]
    }
   ],
   "source": [
    "# Applying polynomial transformation on test data\n",
    "X_test_poly = poly_2.transform(X_test)\n",
    "\n",
    "# Making prediction\n",
    "Y_hat_test = X_test_poly @ W_2\n",
    "\n",
    "# Calculating cost\n",
    "test_cost = np.mean((Y_hat_test - Y_test)**2)\n",
    "print(f\"test_cost: {test_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test cost shows good stability comparing with the validation cost, confirming that the model generalizes well to unseen data. This completes the task of building a **Multiple Non-Linear Regression Model Using Normal Equation**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
